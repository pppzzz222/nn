# TrainingStrategies.py
import os
import math
import pickle
import numpy as np
from datetime import datetime
from collections import deque
import tensorflow as tf
from tensorflow.keras.optimizers import Adam


# è¯¾ç¨‹å­¦ä¹ ç®¡ç†å™¨
class CurriculumManager:
    def __init__(self, env):
        self.env = env
        self.current_stage = 0
        self.stage_thresholds = [0.3, 0.5, 0.7, 0.85]  # æˆåŠŸç‡é˜ˆå€¼
        self.stage_configs = [
            # é˜¶æ®µ0: å…¥é—¨ - æ›´æ—©å¼•å…¥è¡Œäººï¼Œä½†æ•°é‡å°‘é€Ÿåº¦æ…¢
            {
                'pedestrian_cross': 3,      # å¢åŠ ä¸€ç‚¹éš¾åº¦
                'pedestrian_normal': 2,     
                'pedestrian_speed_min': 0.3,  # é™ä½è¡Œäººé€Ÿåº¦
                'pedestrian_speed_max': 0.8,  
                'max_episode_steps': 900,    # å‡å°‘æœ€å¤§æ­¥æ•°ï¼ŒåŠ é€Ÿè®­ç»ƒ
                'success_threshold': 0.3,
                'difficulty_name': 'ç®€å•'
            },
            # é˜¶æ®µ1: åˆçº§
            {
                'pedestrian_cross': 5,      
                'pedestrian_normal': 3,
                'pedestrian_speed_min': 0.5,
                'pedestrian_speed_max': 1.0,
                'max_episode_steps': 1200,   
                'success_threshold': 0.5,
                'difficulty_name': 'åˆçº§'
            },
            # é˜¶æ®µ2: ä¸­çº§ - å¢åŠ ååº”æ—¶é—´è®­ç»ƒ
            {
                'pedestrian_cross': 7,
                'pedestrian_normal': 4,
                'pedestrian_speed_min': 0.7,
                'pedestrian_speed_max': 1.3,
                'max_episode_steps': 1500,   
                'success_threshold': 0.6,  # æé«˜é˜ˆå€¼
                'difficulty_name': 'ä¸­çº§'
            },
            # é˜¶æ®µ3: é«˜çº§
            {
                'pedestrian_cross': 9,
                'pedestrian_normal': 5,
                'pedestrian_speed_min': 0.9,
                'pedestrian_speed_max': 1.6,
                'max_episode_steps': 1800,   
                'success_threshold': 0.7,
                'difficulty_name': 'é«˜çº§'
            },
            # é˜¶æ®µ4: ä¸“å®¶ (æ­£å¸¸éš¾åº¦)
            {
                'pedestrian_cross': 10,     
                'pedestrian_normal': 6,
                'pedestrian_speed_min': 1.0,
                'pedestrian_speed_max': 2.0,
                'max_episode_steps': 2400,
                'success_threshold': 0.8,
                'difficulty_name': 'ä¸“å®¶'
            },
            # é˜¶æ®µ5: å¤§å¸ˆ (æŒ‘æˆ˜)
            {
                'pedestrian_cross': 12,     
                'pedestrian_normal': 8,
                'pedestrian_speed_min': 1.2,
                'pedestrian_speed_max': 2.5,
                'max_episode_steps': 3000,
                'success_threshold': 0.85,
                'difficulty_name': 'å¤§å¸ˆ'
            }
        ]
        
        # è®­ç»ƒå†å²
        self.success_history = deque(maxlen=20)  # è®°å½•æœ€è¿‘20è½®çš„æˆåŠŸæƒ…å†µ
        self.reward_history = deque(maxlen=50)   # è®°å½•æœ€è¿‘50è½®çš„å¥–åŠ±
        self.reaction_time_history = deque(maxlen=50)  # è®°å½•ååº”æ—¶é—´
        
    def update_stage(self, success, reward, reaction_time=None):
        """æ›´æ–°è®­ç»ƒé˜¶æ®µ"""
        # è®°å½•å†å²
        self.success_history.append(1 if success else 0)
        self.reward_history.append(reward)
        if reaction_time is not None:
            self.reaction_time_history.append(reaction_time)
        
        # è®¡ç®—æœ€è¿‘æˆåŠŸç‡
        if len(self.success_history) >= 10:
            success_rate = sum(self.success_history) / len(self.success_history)
            avg_reward = np.mean(self.reward_history) if self.reward_history else 0
            
            # æ¯20è½®æ‰“å°ä¸€æ¬¡
            if len(self.success_history) % 20 == 0:
                stage_info = self.get_current_config()
                print(f"è¯¾ç¨‹å­¦ä¹  - é˜¶æ®µ: {self.current_stage}({stage_info['difficulty_name']}), "
                      f"æˆåŠŸç‡: {success_rate:.2f}, å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
                if self.reaction_time_history:
                    avg_rt = np.mean(self.reaction_time_history)
                    print(f"  å¹³å‡ååº”æ—¶é—´: {avg_rt:.2f}ç§’")
            
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
            if self.current_stage < len(self.stage_configs) - 1:
                next_stage_threshold = self.stage_configs[self.current_stage]['success_threshold']
                
                # ä¸ä»…è¦çœ‹æˆåŠŸç‡ï¼Œè¿˜è¦çœ‹ååº”æ—¶é—´ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                can_advance = success_rate >= next_stage_threshold and avg_reward > 5
                if can_advance and self.reaction_time_history:
                    avg_rt = np.mean(self.reaction_time_history)
                    # è¦æ±‚ååº”æ—¶é—´å°äº1ç§’
                    if avg_rt < 1.0:
                        self.current_stage += 1
                        print(f"ğŸ‰ è¯¾ç¨‹å­¦ä¹ : è¿›é˜¶åˆ°é˜¶æ®µ {self.current_stage}!")
                        print(f"   æ–°é…ç½®: {self.stage_configs[self.current_stage]['difficulty_name']}")
                        return True
                elif can_advance:
                    self.current_stage += 1
                    print(f"ğŸ‰ è¯¾ç¨‹å­¦ä¹ : è¿›é˜¶åˆ°é˜¶æ®µ {self.current_stage}!")
                    print(f"   æ–°é…ç½®: {self.stage_configs[self.current_stage]['difficulty_name']}")
                    return True
                    
            # å¦‚æœè¡¨ç°å¤ªå·®æˆ–ååº”æ—¶é—´å¤ªé•¿ï¼Œé€€å›ä¸Šä¸€é˜¶æ®µ
            if self.current_stage > 0 and (success_rate < 0.2 or 
                (self.reaction_time_history and np.mean(self.reaction_time_history) > 2.0)):
                self.current_stage -= 1
                print(f"âš ï¸ è¯¾ç¨‹å­¦ä¹ : é€€å›é˜¶æ®µ {self.current_stage}")
                return True
        
        return False
    
    def get_current_config(self):
        """è·å–å½“å‰é˜¶æ®µçš„é…ç½®"""
        return self.stage_configs[min(self.current_stage, len(self.stage_configs) - 1)]
    
    def apply_to_environment(self):
        """å°†å½“å‰é˜¶æ®µé…ç½®åº”ç”¨åˆ°ç¯å¢ƒ"""
        config = self.get_current_config()
        return config


# å¤šç›®æ ‡ä¼˜åŒ–å™¨
class MultiObjectiveOptimizer:
    def __init__(self):
        # å®šä¹‰ä¼˜åŒ–ç›®æ ‡åŠå…¶æƒé‡ï¼ˆå¯åŠ¨æ€è°ƒæ•´ï¼‰
        self.objectives = {
            'reaction_time': {
                'weight': 0.25,  # æ–°å¢ï¼šååº”æ—¶é—´æƒé‡
                'description': 'å¿«é€Ÿååº”é¿éšœ',
                'metrics': ['reaction_time', 'proactive_actions']
            },
            'safety': {
                'weight': 0.30,  
                'description': 'å®‰å…¨é¿éšœå’Œé¿å…ç¢°æ’',
                'metrics': ['collision_avoidance', 'pedestrian_distance']
            },
            'efficiency': {
                'weight': 0.25,  
                'description': 'å¿«é€Ÿåˆ°è¾¾ç›®çš„åœ°',
                'metrics': ['progress_speed', 'total_time']
            },
            'comfort': {
                'weight': 0.15,
                'description': 'å¹³ç¨³é©¾é©¶ä½“éªŒ',
                'metrics': ['smoothness', 'steering_changes']
            },
            'rule_following': {
                'weight': 0.05,
                'description': 'éµå®ˆäº¤é€šè§„åˆ™',
                'metrics': ['lane_keeping', 'speed_limit']
            }
        }
        
        # æŒ‡æ ‡è·Ÿè¸ª
        self.metrics_history = {
            'reaction_time': [],
            'safety': [],
            'efficiency': [],
            'comfort': [],
            'rule_following': []
        }
        
    def compute_composite_reward(self, metrics):
        """è®¡ç®—ç»¼åˆå¥–åŠ±å€¼"""
        composite = 0
        
        for obj_name, obj_info in self.objectives.items():
            if obj_name in metrics:
                # å½’ä¸€åŒ–å¤„ç†æ¯ä¸ªç›®æ ‡çš„è´¡çŒ®
                normalized_value = self._normalize_metric(metrics[obj_name], obj_name)
                composite += normalized_value * obj_info['weight']
                
                # è®°å½•æŒ‡æ ‡å†å²
                self.metrics_history[obj_name].append(normalized_value)
        
        # ç‰¹æ®Šå¥–åŠ±/æƒ©ç½šé¡¹
        if metrics.get('collision', False):
            composite -= 10  # å¢åŠ ç¢°æ’æƒ©ç½š
        if metrics.get('off_road', False):
            composite -= 5   # å¢åŠ åç¦»é“è·¯æƒ©ç½š
        if metrics.get('dangerous_action', False):
            composite -= 3   # å¢åŠ å±é™©åŠ¨ä½œæƒ©ç½š
            
        # æ–°å¢ï¼šååº”æ—¶é—´ç›¸å…³å¥–åŠ±/æƒ©ç½š
        if 'reaction_time' in metrics:
            rt = metrics['reaction_time']
            if rt < 0.5:  # å¿«é€Ÿååº”
                composite += 2
            elif rt > 1.5:  # ååº”å¤ªæ…¢
                composite -= 3
        
        # æ–°å¢ï¼šä¸»åŠ¨é¿éšœå¥–åŠ±
        if metrics.get('proactive_action', False):
            composite += 1.5
            
        return composite
    
    def _normalize_metric(self, value, metric_name):
        """å½’ä¸€åŒ–æŒ‡æ ‡å€¼åˆ°[0, 1]èŒƒå›´"""
        # ä¸åŒæŒ‡æ ‡çš„å½’ä¸€åŒ–æ–¹å¼ä¸åŒ
        normalization_rules = {
            'reaction_time': lambda x: max(0, 1 - x/3),  # ååº”æ—¶é—´è¶ŠçŸ­è¶Šå¥½
            'safety': lambda x: min(max(x / 10, 0), 1),
            'efficiency': lambda x: min(max(x / 100, 0), 1),
            'comfort': lambda x: min(max((x + 5) / 10, 0), 1),
            'rule_following': lambda x: min(max(x, 0), 1)
        }
        
        if metric_name in normalization_rules:
            return normalization_rules[metric_name](value)
        return min(max(value, 0), 1)  # é»˜è®¤æˆªæ–­åˆ°[0, 1]
    
    def adjust_weights(self, performance_feedback):
        """æ ¹æ®æ€§èƒ½åé¦ˆåŠ¨æ€è°ƒæ•´æƒé‡"""
        # å¦‚æœæŸä¸ªç›®æ ‡è¡¨ç°æŒç»­è¾ƒå·®ï¼Œå¢åŠ å…¶æƒé‡
        recent_performance = {}
        for obj in self.objectives:
            if len(self.metrics_history[obj]) >= 10:
                recent_avg = np.mean(self.metrics_history[obj][-10:])
                recent_performance[obj] = recent_avg
        
        if recent_performance:
            # æ‰¾åˆ°è¡¨ç°æœ€å·®çš„ç›®æ ‡
            worst_obj = min(recent_performance, key=recent_performance.get)
            best_obj = max(recent_performance, key=recent_performance.get)
            
            # å¦‚æœæœ€å·®ç›®æ ‡è¡¨ç°ä½äºé˜ˆå€¼ï¼Œå¢åŠ å…¶æƒé‡
            if recent_performance[worst_obj] < 0.3:
                adjustment = 0.03  # è°ƒæ•´å¹…åº¦
                self.objectives[worst_obj]['weight'] += adjustment
                self.objectives[best_obj]['weight'] -= adjustment
                
                # ç¡®ä¿æƒé‡æ€»å’Œä¸º1
                total = sum(obj['weight'] for obj in self.objectives.values())
                for obj in self.objectives:
                    self.objectives[obj]['weight'] /= total
                
                if adjustment != 0:
                    print(f"åŠ¨æ€æƒé‡è°ƒæ•´: {worst_obj}æƒé‡â†‘ {adjustment:.3f}, {best_obj}æƒé‡â†“ {adjustment:.3f}")
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = "å¤šç›®æ ‡ä¼˜åŒ–æ€§èƒ½æŠ¥å‘Š:\n"
        report += "=" * 50 + "\n"
        
        for obj_name, obj_info in self.objectives.items():
            history = self.metrics_history[obj_name]
            if history:
                avg = np.mean(history[-20:]) if len(history) >= 20 else np.mean(history)
                report += f"{obj_name}(æƒé‡:{obj_info['weight']:.2f}): å¹³å‡å¾—åˆ†={avg:.3f}\n"
                report += f"  æè¿°: {obj_info['description']}\n"
        
        return report


# æ¨¡ä»¿å­¦ä¹ ç®¡ç†å™¨ï¼ˆä¿æŒä¸å˜ï¼Œç•¥ä½œä¿®æ”¹ï¼‰
class ImitationLearningManager:
    def __init__(self, expert_data_path=None):
        self.expert_data_path = expert_data_path
        self.expert_data = []
        self.is_pretrained = False
        
    def load_expert_data(self, path):
        """åŠ è½½ä¸“å®¶ç¤ºèŒƒæ•°æ®"""
        try:
            if os.path.exists(path):
                with open(path, 'rb') as f:
                    self.expert_data = pickle.load(f)
                print(f"å·²åŠ è½½ {len(self.expert_data)} æ¡ä¸“å®¶ç¤ºèŒƒæ•°æ®")
                return True
            else:
                print(f"ä¸“å®¶æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False
        except Exception as e:
            print(f"åŠ è½½ä¸“å®¶æ•°æ®å¤±è´¥: {e}")
            return False


# ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆå¢åŠ å¯¹å±é™©ç»éªŒçš„ä¼˜å…ˆçº§ï¼‰
class PrioritizedReplayBuffer:
    def __init__(self, max_size=15000, alpha=0.7, beta_start=0.5, beta_frames=50000):
        self.max_size = max_size
        self.alpha = alpha  # ä¼˜å…ˆçº§ç¨‹åº¦ (0 = å‡åŒ€é‡‡æ ·, 1 = å®Œå…¨ä¼˜å…ˆçº§)
        self.beta_start = beta_start  # é‡è¦æ€§é‡‡æ ·æƒé‡èµ·å§‹å€¼
        self.beta_frames = beta_frames  # betaçº¿æ€§å¢é•¿çš„å¸§æ•°
        self.frame = 1
        
        # ä½¿ç”¨å¾ªç¯ç¼“å†²åŒº
        self.buffer = deque(maxlen=max_size)
        self.priorities = deque(maxlen=max_size)
        
    def __len__(self):
        return len(self.buffer)
    
    def beta(self):
        """çº¿æ€§é€’å¢çš„betaå€¼ï¼Œç”¨äºé‡è¦æ€§é‡‡æ ·æƒé‡"""
        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)
    
    def add(self, experience, error=None):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        if error is None:
            priority = max(self.priorities) if self.priorities else 1.0
        else:
            priority = (abs(error) + 1e-5) ** self.alpha
            
        # å¦‚æœæ˜¯å±é™©ç»éªŒï¼ˆè´Ÿå¥–åŠ±è¾ƒå¤§ï¼‰ï¼Œå¢åŠ ä¼˜å…ˆçº§
        reward = experience[2]
        if reward < -2:  # å±é™©ç»éªŒ
            priority *= 1.5
            
        self.buffer.append(experience)
        self.priorities.append(priority)
        
    def sample(self, batch_size):
        """ä»ç¼“å†²åŒºä¸­é‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        if len(self.buffer) == 0:
            return [], [], []
            
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        priorities = np.array(self.priorities, dtype=np.float32)
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), p=probs, replace=False)
        
        # è·å–æ ·æœ¬
        samples = [self.buffer[i] for i in indices]
        
        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta())
        weights /= weights.max()  # å½’ä¸€åŒ–
        
        # æ›´æ–°å¸§è®¡æ•°å™¨
        self.frame += 1
        
        return indices, samples, weights
    
    def update_priorities(self, indices, errors):
        """æ›´æ–°é‡‡æ ·ç»éªŒçš„ä¼˜å…ˆçº§"""
        for idx, error in zip(indices, errors):
            if 0 <= idx < len(self.priorities):
                self.priorities[idx] = (abs(error) + 1e-5) ** self.alpha