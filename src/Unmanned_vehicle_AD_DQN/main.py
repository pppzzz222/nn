# main_fixed.py - ä¿®å¤æ¨¡å‹ä¿å­˜é—®é¢˜çš„ç‰ˆæœ¬
import glob
import os
import sys
import random
import time
import numpy as np
import cv2
import math
import matplotlib.pyplot as plt
from collections import deque
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import tensorflow.keras.backend as backend
from threading import Thread

from tqdm import tqdm
import pickle

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from Environment import CarEnv
from Model import DQNAgent
from TrainingStrategies import CurriculumManager, MultiObjectiveOptimizer, ImitationLearningManager
import Hyperparameters

# ä»Hyperparameterså¯¼å…¥æ‰€æœ‰å‚æ•°
from Hyperparameters import *

def ensure_models_directory():
    """ç¡®ä¿modelsç›®å½•å­˜åœ¨"""
    if not os.path.exists('models'):
        os.makedirs('models')
        print("âœ… å·²åˆ›å»º models ç›®å½•")
    return 'models'

def save_model_with_retry(model, filepath, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æ¨¡å‹ä¿å­˜"""
    for attempt in range(max_retries):
        try:
            model.save(filepath)
            print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {os.path.basename(filepath)}")
            return True
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å¤±è´¥ (å°è¯• {attempt+1}/{max_retries}): {e}")
            time.sleep(1)
    
    print(f"âŒ æ— æ³•ä¿å­˜æ¨¡å‹: {filepath}")
    return False

def extended_reward_calculation(env, action, reward, done, step_info):
    """
    æ‰©å±•çš„å¥–åŠ±è®¡ç®—å‡½æ•°ï¼Œç”¨äºå¤šç›®æ ‡ä¼˜åŒ–
    """
    # è·å–è½¦è¾†çŠ¶æ€
    vehicle_location = env.vehicle.get_location()
    velocity = env.vehicle.get_velocity()
    speed_kmh = 3.6 * math.sqrt(velocity.x**2 + velocity.y**2)
    
    # è®¡ç®—å¤šç›®æ ‡æŒ‡æ ‡
    metrics = {}
    
    # 1. ååº”æ—¶é—´æŒ‡æ ‡
    reaction_time = 0
    if hasattr(env, 'obstacle_detected_time') and env.obstacle_detected_time is not None:
        if hasattr(env, 'reaction_start_time') and env.reaction_start_time is not None:
            reaction_time = time.time() - env.reaction_start_time
    
    metrics['reaction_time'] = reaction_time
    
    # 2. ä¸»åŠ¨é¿éšœæŒ‡æ ‡
    proactive_action = False
    if hasattr(env, 'suggested_action') and env.suggested_action is not None:
        if action == env.suggested_action:
            proactive_action = True
    
    metrics['proactive_action'] = proactive_action
    
    # 3. å®‰å…¨æ€§æŒ‡æ ‡ - åŸºäºæœ€è¿‘è¡Œäººè·ç¦»
    min_ped_distance = getattr(env, 'last_ped_distance', float('inf'))
    safety_score = 0
    if min_ped_distance < 100:
        if min_ped_distance > 12:
            safety_score = 10  # éå¸¸å®‰å…¨
        elif min_ped_distance > 8:
            safety_score = 7   # å®‰å…¨
        elif min_ped_distance > 5:
            safety_score = 3   # è­¦å‘Š
        elif min_ped_distance > 3:
            safety_score = 1   # å±é™©
        else:
            safety_score = 0   # æå±é™©
    
    metrics['safety'] = safety_score
    
    # 4. æ•ˆç‡æŒ‡æ ‡ - åŸºäºè¿›åº¦
    progress = (vehicle_location.x + 81) / 236.0  # ä»-81åˆ°155
    efficiency_score = progress * 100  # è¿›åº¦ç™¾åˆ†æ¯”
    metrics['efficiency'] = efficiency_score
    
    # 5. èˆ’é€‚åº¦æŒ‡æ ‡ - åŸºäºè½¬å‘å¹³æ»‘æ€§
    comfort_score = 5  # é»˜è®¤èˆ’é€‚
    
    if hasattr(env, 'last_action') and env.last_action in [3, 4]:
        if getattr(env, 'same_steer_counter', 0) > 2:  # è¿ç»­åŒå‘è½¬å‘
            comfort_score = 2   # ç¨ä¸èˆ’é€‚
        elif getattr(env, 'same_steer_counter', 0) > 1:
            comfort_score = 3   # ä¸€èˆ¬
        else:
            comfort_score = 4   # èˆ’é€‚
    else:
        comfort_score = 5  # ç›´è¡Œï¼Œæœ€èˆ’é€‚
    
    metrics['comfort'] = comfort_score
    
    # 6. è§„åˆ™éµå¾ªæŒ‡æ ‡ - åŸºäºé€Ÿåº¦
    rule_score = 0.3  # é»˜è®¤è¾ƒä½åˆ†æ•°
    
    if 20 <= speed_kmh <= 35:  # ç†æƒ³é€Ÿåº¦èŒƒå›´
        rule_score = 1.0
    elif 15 <= speed_kmh < 20 or 35 < speed_kmh <= 40:
        rule_score = 0.7
    elif 10 <= speed_kmh < 15 or 40 < speed_kmh <= 45:
        rule_score = 0.5
    elif 5 <= speed_kmh < 10:
        rule_score = 0.4
    
    metrics['rule_following'] = rule_score
    
    # 7. ç‰¹æ®Šäº‹ä»¶
    metrics['collision'] = len(getattr(env, 'collision_history', [])) > 0
    metrics['off_road'] = vehicle_location.x < -90 or abs(vehicle_location.y + 195) > 30
    
    # 8. å±é™©åŠ¨ä½œæ£€æµ‹
    if speed_kmh > 40 and action in [3, 4]:  # é«˜é€Ÿæ€¥è½¬
        metrics['dangerous_action'] = True
    else:
        metrics['dangerous_action'] = False
    
    return metrics

if __name__ == '__main__':
    FPS = 60  # å¸§ç‡
    ep_rewards = [-200]  # å­˜å‚¨æ¯è½®å¥–åŠ±

    # ç¡®ä¿modelsç›®å½•å­˜åœ¨
    models_dir = ensure_models_directory()
    
    # GPUå†…å­˜é…ç½®
    gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)
    tf.compat.v1.keras.backend.set_session(
        tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options)))

    # åˆ›å»ºæ™ºèƒ½ä½“å’Œç¯å¢ƒ
    agent = DQNAgent(
        use_dueling=True, 
        use_per=True,
        use_curriculum=True,
        use_multi_objective=True
    )
    
    env = CarEnv()
    
    # è®¾ç½®è®­ç»ƒç­–ç•¥
    agent.setup_training_strategies(env)

    # å¯åŠ¨è®­ç»ƒçº¿ç¨‹å¹¶ç­‰å¾…è®­ç»ƒåˆå§‹åŒ–å®Œæˆ
    trainer_thread = Thread(target=agent.train_in_loop, daemon=True)
    trainer_thread.start()
    while not agent.training_initialized:
        time.sleep(0.01)

    # é¢„çƒ­Qç½‘ç»œ
    agent.get_qs(np.ones((env.im_height, env.im_width, 3)))

    # è®­ç»ƒç»Ÿè®¡å˜é‡
    best_score = -float('inf')  # æœ€ä½³å¾—åˆ†
    success_count = 0  # æˆåŠŸæ¬¡æ•°è®¡æ•°
    scores = []  # å­˜å‚¨æ¯è½®å¾—åˆ†
    avg_scores = []  # å­˜å‚¨å¹³å‡å¾—åˆ†
    
    # è®°å½•PERç›¸å…³ç»Ÿè®¡
    per_stats = {
        'avg_td_error': [],
        'buffer_size': []
    }
    
    # å¤šç›®æ ‡ç»Ÿè®¡
    multi_obj_stats = {
        'reaction_time': [],
        'safety': [],
        'efficiency': [],
        'comfort': [],
        'rule_following': []
    }
    
    # è¯¾ç¨‹å­¦ä¹ é˜¶æ®µè®°å½•
    curriculum_stages = []
    
    # ååº”æ—¶é—´ç»Ÿè®¡
    reaction_time_stats = []
    
    # è¿­ä»£è®­ç»ƒè½®æ¬¡
    epds = []
    for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):
        env.collision_hist = []  # é‡ç½®ç¢°æ’å†å²
        agent.tensorboard.step = episode  # è®¾ç½®TensorBoardæ­¥æ•°

        # åº”ç”¨è¯¾ç¨‹å­¦ä¹ é…ç½®
        if agent.curriculum_manager:
            config = agent.curriculum_manager.get_current_config()
            if episode % 50 == 0:  # æ¯50è½®æ‰“å°ä¸€æ¬¡
                print(f"è¯¾ç¨‹å­¦ä¹  - é˜¶æ®µ {agent.curriculum_manager.current_stage}({config['difficulty_name']}): "
                      f"è¡Œäºº(åå­—è·¯å£={config['pedestrian_cross']}, æ™®é€š={config['pedestrian_normal']})")
            curriculum_stages.append(agent.curriculum_manager.current_stage)
        
        # é‡ç½®æ¯è½®ç»Ÿè®¡
        score = 0
        step = 1
        
        # å¤šç›®æ ‡æŒ‡æ ‡è®°å½•
        episode_metrics = {
            'reaction_time': [],
            'safety': [],
            'efficiency': [],
            'comfort': [],
            'rule_following': []
        }

        # é‡ç½®ç¯å¢ƒå¹¶è·å–åˆå§‹çŠ¶æ€
        current_state = env.reset(episode)

        # é‡ç½®å®Œæˆæ ‡å¿—
        done = False
        episode_start = time.time()

        # åº”ç”¨è¯¾ç¨‹å­¦ä¹ çš„æœ€å¤§æ­¥æ•°é™åˆ¶
        if agent.curriculum_manager:
            config = agent.curriculum_manager.get_current_config()
            max_steps_per_episode = config['max_episode_steps']
        else:
            max_steps_per_episode = SECONDS_PER_EPISODE * FPS

        # ä»…åœ¨ç»™å®šç§’æ•°å†…è¿è¡Œ
        while not done and step < max_steps_per_episode:
            # é€‰æ‹©åŠ¨ä½œç­–ç•¥
            if np.random.random() > Hyperparameters.EPSILON:
                # ä»Qç½‘ç»œè·å–åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰
                qs = agent.get_qs(current_state)
                action = np.argmax(qs)
                
                # å¦‚æœæœ‰å»ºè®®çš„é¿è®©åŠ¨ä½œï¼Œè€ƒè™‘é‡‡çº³
                if hasattr(env, 'suggested_action') and env.suggested_action is not None:
                    # æ£€æŸ¥å»ºè®®åŠ¨ä½œçš„Qå€¼
                    suggested_q = qs[env.suggested_action]
                    current_best_q = qs[action]
                    
                    # å¦‚æœå»ºè®®åŠ¨ä½œçš„Qå€¼æ¥è¿‘æœ€ä½³åŠ¨ä½œï¼Œé‡‡çº³å»ºè®®
                    if suggested_q > current_best_q * 0.8:
                        action = env.suggested_action
                
                if episode % 100 == 0 and step % 100 == 0:  # å‡å°‘æ‰“å°é¢‘ç‡
                    print(f'Ep {episode} Step {step}: Qå€¼ [{qs[0]:>5.2f}, {qs[1]:>5.2f}, {qs[2]:>5.2f}, {qs[3]:>5.2f}, {qs[4]:>5.2f}] åŠ¨ä½œ: {action}')
            else:
                # éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰
                action = np.random.randint(0, 5)
                # æ·»åŠ å»¶è¿Ÿä»¥åŒ¹é…60FPS
                time.sleep(1 / FPS)

            # æ‰§è¡ŒåŠ¨ä½œå¹¶è·å–ç»“æœ
            new_state, reward, done, _ = env.step(action)
            
            # è®¡ç®—ååº”æ—¶é—´
            reaction_time = 0
            if hasattr(env, 'obstacle_detected_time') and env.obstacle_detected_time is not None:
                if hasattr(env, 'reaction_start_time') and env.reaction_start_time is not None:
                    reaction_time = time.time() - env.reaction_start_time
            
            # è®¡ç®—å¤šç›®æ ‡æŒ‡æ ‡
            if agent.multi_objective_optimizer:
                step_info = {'step': step, 'action': action}
                metrics = extended_reward_calculation(env, action, reward, done, step_info)
                
                # è®°å½•æŒ‡æ ‡
                for key in episode_metrics:
                    if key in metrics:
                        episode_metrics[key].append(metrics[key])
                
                # ä½¿ç”¨å¤šç›®æ ‡ä¼˜åŒ–å™¨è®¡ç®—ç»¼åˆå¥–åŠ±
                composite_reward = agent.multi_objective_optimizer.compute_composite_reward(metrics)
                reward = composite_reward  # ä½¿ç”¨ç»¼åˆå¥–åŠ±
            
            score += reward  # ç´¯åŠ å¥–åŠ±
            
            # æ›´æ–°ç»éªŒå›æ”¾ï¼ˆå¸¦ååº”æ—¶é—´ä¿¡æ¯ï¼‰
            agent.update_replay_memory((current_state, action, reward, new_state, done), 
                                      reaction_time=reaction_time)
            current_state = new_state  # æ›´æ–°å½“å‰çŠ¶æ€

            step += 1

            if done:
                break

        # æœ¬è½®ç»“æŸ - é”€æ¯æ‰€æœ‰actor
        env.cleanup_actors()
        
        # è®¡ç®—å¹³å‡ååº”æ—¶é—´
        if episode_metrics['reaction_time']:
            avg_reaction_time = np.mean([rt for rt in episode_metrics['reaction_time'] if rt > 0])
            reaction_time_stats.append(avg_reaction_time)
        
        # è®¡ç®—æœ¬è½®å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        for key, values in episode_metrics.items():
            if values:
                # è¿‡æ»¤æ‰é›¶å€¼ï¼ˆæ— ååº”æ—¶ï¼‰
                if key == 'reaction_time':
                    filtered_values = [v for v in values if v > 0]
                    avg_metrics[key] = np.mean(filtered_values) if filtered_values else 0
                else:
                    avg_metrics[key] = np.mean(values)
                # è®°å½•åˆ°ç»Ÿè®¡ä¸­
                if key in multi_obj_stats:
                    multi_obj_stats[key].append(avg_metrics[key])
        
        # æ›´æ–°è¯¾ç¨‹å­¦ä¹ ï¼ˆå¸¦ååº”æ—¶é—´ï¼‰
        success = score > 5  # æˆåŠŸå®Œæˆçš„é˜ˆå€¼
        avg_rt = avg_metrics.get('reaction_time', 0)
        if agent.curriculum_manager:
            stage_changed = agent.curriculum_manager.update_stage(success, score, avg_rt)
            if stage_changed:
                print(f"è¯¾ç¨‹å­¦ä¹ é˜¶æ®µå·²æ›´æ–°: {agent.curriculum_manager.current_stage}")
                
                # é˜¶æ®µå˜åŒ–æ—¶ä¿å­˜æ¨¡å‹
                model_path = f'{models_dir}/{MODEL_NAME}_stage_{agent.curriculum_manager.current_stage}_ep_{episode}.model'
                save_model_with_retry(agent.model, model_path)
        
        # æ›´æ–°å¤šç›®æ ‡ä¼˜åŒ–å™¨æƒé‡
        if agent.multi_objective_optimizer and episode % 20 == 0:
            agent.multi_objective_optimizer.adjust_weights(avg_metrics)
        
        # æ›´æ–°æˆåŠŸè®¡æ•°
        if success:
            success_count += 1
        
        # ============================================
        # ä¿®å¤ï¼šç®€åŒ–æ¨¡å‹ä¿å­˜æ¡ä»¶
        # ============================================
        
        # 1. å®šæœŸä¿å­˜æ¨¡å‹ï¼ˆæ¯10è½®ï¼‰
        if episode % 10 == 0:
            model_path = f'{models_dir}/{MODEL_NAME}_ep{episode}_score{score:.1f}.model'
            save_model_with_retry(agent.model, model_path)
        
        # 2. ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœæ¯”ä¹‹å‰å¥½ï¼‰
        if score > best_score:
            best_score = score
            model_path = f'{models_dir}/{MODEL_NAME}_best_ep{episode}_score{score:.1f}.model'
            save_model_with_retry(agent.model, model_path)
            print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹: Episode {episode}, å¾—åˆ†: {score:.2f}")
        
        # 3. ä¿å­˜é‡Œç¨‹ç¢‘æ¨¡å‹ï¼ˆæ¯50è½®ï¼‰
        if episode % 50 == 0:
            model_path = f'{models_dir}/{MODEL_NAME}_milestone_ep{episode}.model'
            save_model_with_retry(agent.model, model_path)
            print(f"ğŸ¯ é‡Œç¨‹ç¢‘æ¨¡å‹: Episode {episode}")
        
        # è®°å½•å¾—åˆ†ç»Ÿè®¡
        scores.append(score)
        avg_scores.append(np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores))

        # è®°å½•PERç¼“å†²åŒºä¿¡æ¯
        if hasattr(agent, 'replay_buffer'):
            per_stats['buffer_size'].append(len(agent.replay_buffer))

        # å®šæœŸèšåˆç»Ÿè®¡ä¿¡æ¯
        if not episode % AGGREGATE_STATS_EVERY or episode == 1:
            average_reward = np.mean(scores[-AGGREGATE_STATS_EVERY:]) if len(scores) >= AGGREGATE_STATS_EVERY else np.mean(scores)
            min_reward = min(scores[-AGGREGATE_STATS_EVERY:]) if len(scores) >= AGGREGATE_STATS_EVERY else min(scores)
            max_reward = max(scores[-AGGREGATE_STATS_EVERY:]) if len(scores) >= AGGREGATE_STATS_EVERY else max(scores)
            
            # æ·»åŠ PERç»Ÿè®¡åˆ°TensorBoard
            stats_dict = {
                'reward_avg': average_reward, 
                'reward_min': min_reward, 
                'reward_max': max_reward,
                'epsilon': Hyperparameters.EPSILON
            }
            
            if hasattr(agent, 'replay_buffer'):
                avg_buffer = np.mean(per_stats['buffer_size'][-AGGREGATE_STATS_EVERY:]) if per_stats['buffer_size'] else 0
                stats_dict['buffer_size'] = avg_buffer
            
            # æ·»åŠ å¤šç›®æ ‡æŒ‡æ ‡
            if agent.multi_objective_optimizer:
                for obj in ['reaction_time', 'safety', 'efficiency', 'comfort', 'rule_following']:
                    if multi_obj_stats[obj]:
                        recent_avg = np.mean(multi_obj_stats[obj][-AGGREGATE_STATS_EVERY:]) if len(multi_obj_stats[obj]) >= AGGREGATE_STATS_EVERY else np.mean(multi_obj_stats[obj])
                        stats_dict[f'{obj}_score'] = recent_avg
            
            # æ·»åŠ ååº”æ—¶é—´ç»Ÿè®¡
            if reaction_time_stats:
                avg_rt = np.mean(reaction_time_stats[-AGGREGATE_STATS_EVERY:]) if len(reaction_time_stats) >= AGGREGATE_STATS_EVERY else np.mean(reaction_time_stats)
                stats_dict['reaction_time'] = avg_rt
            
            agent.tensorboard.update_stats(**stats_dict)

        epds.append(episode)
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if episode % 10 == 0:  # æ¯10è½®æ‰“å°ä¸€æ¬¡
            avg_rt = np.mean(reaction_time_stats[-10:]) if len(reaction_time_stats) >= 10 else 0
            info_str = f'è½®æ¬¡: {episode:3d}, å¾—åˆ†: {score:6.2f}, æˆåŠŸ: {success_count:3d}, ååº”æ—¶é—´: {avg_rt:.2f}s'
            if agent.curriculum_manager:
                info_str += f', é˜¶æ®µ: {agent.curriculum_manager.current_stage}'
            print(info_str)
        
        # è¡°å‡æ¢ç´¢ç‡
        if Hyperparameters.EPSILON > Hyperparameters.MIN_EPSILON:
            Hyperparameters.EPSILON *= Hyperparameters.EPSILON_DECAY
            Hyperparameters.EPSILON = max(Hyperparameters.MIN_EPSILON, Hyperparameters.EPSILON)

    # è®¾ç½®è®­ç»ƒçº¿ç¨‹ç»ˆæ­¢æ ‡å¿—å¹¶ç­‰å¾…å…¶ç»“æŸ
    agent.terminate = True
    trainer_thread.join()
    
    # ============================================
    # ä¿®å¤ï¼šå§‹ç»ˆä¿å­˜æœ€ç»ˆæ¨¡å‹
    # ============================================
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = f'{models_dir}/{MODEL_NAME}_final_ep{EPISODES}_avg{np.mean(scores):.1f}.model'
    save_model_with_retry(agent.model, final_model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # åŒæ—¶ä¿å­˜ç›®æ ‡ç½‘ç»œ
    final_target_path = f'{models_dir}/{MODEL_NAME}_target_final.model'
    save_model_with_retry(agent.target_model, final_target_path)
    print(f"âœ… ç›®æ ‡ç½‘ç»œå·²ä¿å­˜: {final_target_path}")
    
    # ä¿å­˜è®­ç»ƒç»Ÿè®¡æ•°æ®
    training_stats = {
        'scores': scores,
        'avg_scores': avg_scores,
        'multi_obj_stats': multi_obj_stats,
        'reaction_time_stats': reaction_time_stats,
        'curriculum_stages': curriculum_stages,
        'final_scores': {
            'max': max(scores) if scores else 0,
            'avg': np.mean(scores) if scores else 0,
            'min': min(scores) if scores else 0,
        }
    }
    
    stats_file = f'training_stats_{int(time.time())}.pkl'
    with open(stats_file, 'wb') as f:
        pickle.dump(training_stats, f)
    print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {stats_file}")
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆ!")
    print("="*60)
    print(f"æœ€ç»ˆç»Ÿè®¡:")
    print(f"  æ€»è½®æ¬¡: {EPISODES}")
    print(f"  æœ€ä½³å¾—åˆ†: {max(scores) if scores else 0:.2f}")
    print(f"  å¹³å‡å¾—åˆ†: {np.mean(scores) if scores else 0:.2f}")
    print(f"  æˆåŠŸç‡: {(success_count/EPISODES)*100:.1f}%")
    print(f"  å¹³å‡ååº”æ—¶é—´: {np.mean(reaction_time_stats) if reaction_time_stats else 0:.2f}ç§’")
    print(f"  æœ€ç»ˆæ¢ç´¢ç‡: {Hyperparameters.EPSILON:.4f}")
    
    # æ˜¾ç¤ºä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
    print(f"\nå·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶:")
    model_files = glob.glob(f'{models_dir}/*.model')
    if model_files:
        for model_file in sorted(model_files, key=os.path.getmtime):
            file_size = os.path.getsize(model_file) / (1024 * 1024)  # MB
            print(f"  ğŸ“ {os.path.basename(model_file)} ({file_size:.1f} MB)")
    else:
        print("  âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ä¿å­˜è·¯å¾„å’Œæƒé™")